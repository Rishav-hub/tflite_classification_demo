{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, load you model and allocate the tensors\n",
    "import tensorflow.lite as tflite\n",
    "# import tflite_runtime.interpreter as tflite\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tflite.Interpreter(model_path='../data/model_dir/model.tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allocate the tensors\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get the input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'input_1',\n",
       "  'index': 0,\n",
       "  'shape': array([  1, 224, 224,   3]),\n",
       "  'shape_signature': array([ -1, 224, 224,   3]),\n",
       "  'dtype': numpy.uint8,\n",
       "  'quantization': (0.003921568859368563, 0),\n",
       "  'quantization_parameters': {'scales': array([0.00392157], dtype=float32),\n",
       "   'zero_points': array([0]),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Identity',\n",
       "  'index': 168,\n",
       "  'shape': array([1, 5]),\n",
       "  'shape_signature': array([-1,  5]),\n",
       "  'dtype': numpy.uint8,\n",
       "  'quantization': (0.00390625, 0),\n",
       "  'quantization_parameters': {'scales': array([0.00390625], dtype=float32),\n",
       "   'zero_points': array([0]),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(\"d:/\",\"oneNeuron/Source/tflite_testing/daisy.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.isfile(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import imghdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if imghdr.what(image_path) not in [\"jpeg\", \"jpg\", \"png\"]:\n",
    "    print(\"quiting the program\")\n",
    "img = cv2.imread(image_path)\n",
    "img = cv2.resize(img,(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1, 224, 224,   3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = input_details[0]['shape']\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[  5, 117,  83],\n",
       "         [  6, 116,  82],\n",
       "         [  7, 114,  81],\n",
       "         ...,\n",
       "         [ 12,  83,  10],\n",
       "         [  7,  78,   5],\n",
       "         [  2,  74,   0]],\n",
       "\n",
       "        [[  5, 117,  83],\n",
       "         [  5, 116,  82],\n",
       "         [  6, 113,  80],\n",
       "         ...,\n",
       "         [ 11,  82,   9],\n",
       "         [  7,  78,   5],\n",
       "         [  3,  74,   1]],\n",
       "\n",
       "        [[  5, 117,  82],\n",
       "         [  4, 115,  81],\n",
       "         [  6, 113,  80],\n",
       "         ...,\n",
       "         [ 10,  82,   9],\n",
       "         [  6,  78,   5],\n",
       "         [  3,  74,   1]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[  6,  95,  26],\n",
       "         [  5,  95,  28],\n",
       "         [  5,  95,  29],\n",
       "         ...,\n",
       "         [  3,  50,   3],\n",
       "         [  3,  50,   3],\n",
       "         [  3,  49,   3]],\n",
       "\n",
       "        [[  6,  94,  25],\n",
       "         [  5,  95,  27],\n",
       "         [  4,  94,  28],\n",
       "         ...,\n",
       "         [  4,  50,   4],\n",
       "         [  3,  50,   3],\n",
       "         [  3,  49,   3]],\n",
       "\n",
       "        [[  5,  94,  24],\n",
       "         [  5,  94,  27],\n",
       "         [  4,  94,  28],\n",
       "         ...,\n",
       "         [  4,  50,   4],\n",
       "         [  3,  50,   3],\n",
       "         [  3,  49,   3]]]], dtype=uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor= np.array(np.expand_dims(img,0))\n",
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the tensor to point to the input data to be inferred, then run the inference\n",
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "input_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Identity',\n",
       "  'index': 168,\n",
       "  'shape': array([1, 5]),\n",
       "  'shape_signature': array([-1,  5]),\n",
       "  'dtype': numpy.uint8,\n",
       "  'quantization': (0.00390625, 0),\n",
       "  'quantization_parameters': {'scales': array([0.00390625], dtype=float32),\n",
       "   'zero_points': array([0]),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpreter.set_tensor(input_index, input_tensor)\n",
    "interpreter.invoke()\n",
    "output_details = interpreter.get_output_details()\n",
    "output_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([237,   3,   2,   5,   8], dtype=uint8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make the prediction\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "pred = np.squeeze(output_data)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [237   3   2   5   8]\n"
     ]
    }
   ],
   "source": [
    "# If you print “pred”, you will get an array of uint8 (0–255) values. These correspond to the confidence for each class. In my example that would be the 16 bird species we specified in the Collecting Image Data For Machine Learning in Python tutorial.\n",
    "# To get a more human friendly prediction, specify the class indices below.\n",
    "class_ind = {\n",
    "  0: \"daisy\",\n",
    "  1: \"dandelion\",\n",
    "  2: \"roses\",\n",
    "  3: \"sunflowers\",\n",
    "  4: \"tulips\",}\n",
    "# # Since “pred” corresponds to the confidence of each bird class, our prediction should be the highest value in the array. Grab the highest prediction location.\n",
    "highest_pred_loc = np.argmax(pred)\n",
    "print(highest_pred_loc, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted flower is daisy\n"
     ]
    }
   ],
   "source": [
    "# # Use the “highest_pred_loc” to search the “class_ind” dictionary for the actual bird’s name. Print out the prediction and you’re done!\n",
    "flower_name = class_ind[highest_pred_loc]\n",
    "print(f\"predicted flower is {flower_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.lite as tflite\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import imghdr\n",
    "import sys\n",
    "\n",
    "def predict(config, label_map):\n",
    "    # model_path = config[\"train\"][\"model_path\"]\n",
    "    img_path = config[\"predict\"][\"test_img_path\"]\n",
    "    interpreter = tflite.Interpreter(model_path=\"../data/model_dir/model.tflite\")\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    img_type = imghdr.what(img_path)\n",
    "    a_file = os.path.isfile(img_path)\n",
    "    if (img_type not in [\"jpeg\", \"png\", \"jpg\"]) or (not a_file):\n",
    "        print(\"quiting the program\")\n",
    "        sys.exit()\n",
    "    else:\n",
    "        print(f\"yes the file is of type: {img_type}. proceeding further\")\n",
    "    batch_size, pixel_ht, pexel_wd, ch = input_details[0]['shape']\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (pixel_ht, pexel_wd))\n",
    "    input_img = np.expand_dims(img, 0)\n",
    "    input_index = input_details[0]['index']\n",
    "    interpreter.set_tensor(input_index, input_img)\n",
    "    interpreter.invoke()\n",
    "    output_index = output_details[0]['index']\n",
    "    output = interpreter.get_tensor(output_index)\n",
    "    pred =  np.squeeze(output)\n",
    "    argmax = np.argmax(pred)\n",
    "    result = label_map[argmax]\n",
    "    return result, argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.common import read_yaml\n",
    "\n",
    "config = read_yaml(\"../config/config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "  0: \"daisy\",\n",
    "  1: \"dandelion\",\n",
    "  2: \"roses\",\n",
    "  3: \"sunflowers\",\n",
    "  4: \"tulips\",}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes the file is of type: jpeg. proceeding further\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('daisy', 0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(config, label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': {'data_dir': 'data',\n",
       "  'data_split': 0.9,\n",
       "  'model_file_dir': 'data/model_dir',\n",
       "  'data_file_name': 'flower_photos.tgz',\n",
       "  'base_url': 'https://storage.googleapis.com/download.tensorflow.org/example_images/'},\n",
       " 'predict': {'test_img': 'd:/oneNeuron/Source/tflite_testing/daisy.jpeg'}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': {'data_dir': 'data',\n",
       "  'data_split': 0.9,\n",
       "  'model_file_dir': 'data/model_dir',\n",
       "  'data_file_name': 'flower_photos.tgz',\n",
       "  'base_url': 'https://storage.googleapis.com/download.tensorflow.org/example_images/'},\n",
       " 'predict': {'test_img': 'd:/oneNeuron/Source/tflite_testing/daisy.jpeg'}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_yaml(Path(\"../config/config.yaml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  5, 117,  83],\n",
       "        [  5, 117,  83],\n",
       "        [  6, 116,  82],\n",
       "        ...,\n",
       "        [  9,  80,   7],\n",
       "        [  5,  76,   3],\n",
       "        [  2,  73,   0]],\n",
       "\n",
       "       [[  5, 117,  83],\n",
       "        [  4, 116,  82],\n",
       "        [  5, 115,  81],\n",
       "        ...,\n",
       "        [  9,  80,   7],\n",
       "        [  5,  76,   3],\n",
       "        [  3,  74,   1]],\n",
       "\n",
       "       [[  4, 116,  82],\n",
       "        [  4, 116,  82],\n",
       "        [  5, 115,  81],\n",
       "        ...,\n",
       "        [  6,  79,   6],\n",
       "        [  4,  77,   4],\n",
       "        [  1,  74,   1]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  5,  96,  27],\n",
       "        [  5,  95,  29],\n",
       "        [  5,  95,  30],\n",
       "        ...,\n",
       "        [  3,  51,   3],\n",
       "        [  2,  50,   2],\n",
       "        [  2,  50,   2]],\n",
       "\n",
       "       [[  6,  95,  25],\n",
       "        [  6,  95,  25],\n",
       "        [  5,  95,  29],\n",
       "        ...,\n",
       "        [  4,  50,   4],\n",
       "        [  3,  49,   3],\n",
       "        [  3,  49,   3]],\n",
       "\n",
       "       [[  5,  94,  24],\n",
       "        [  6,  95,  25],\n",
       "        [  4,  94,  28],\n",
       "        ...,\n",
       "        [  4,  50,   4],\n",
       "        [  3,  49,   3],\n",
       "        [  3,  49,   3]]], dtype=uint8)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imread(config[\"predict\"][\"test_img\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14c16cc4f6a40c7a6121611b305c8e897dd0b5e91a5f57518321cb656d748a9e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
